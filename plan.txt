My current plan is to use the "outcome" field as the required thing to look for in the abstract.

Basically, for each outcome I need some distribution of values it can have.

What I need to do is grab ALL the outcomes.

Then, I really need to sort out which are the most common.
What I get out will be some distribution. I think I need at least 5 outcomes to get a good idea of how the outcome could be categorized.
The number of categories is certainly tricky. It may need to depend on the outcome. Some may be binary, others may simply be a predicted quantitative result.


It's also KEY to take a look at how much I can get out of the abstracts... If I can get a sense of how many abstracts contain information sufficient to categorize the outcomes, then I can see if I should just use abstracts, or if I really need to dive into the papers. OR, to see if I should use a mix of both?

Both of these would benefit from a full scraping of the 3ie website.


First 150: (recent papers)
Statistics:
Mean number of outcomes per record: 3.93
Median number of outcomes per record: 2.00
Number of records with only 1 outcome: 42


I am probably being mislead also by the fact things are newer and thus probably more available/better...


So a lot of these records clearly will not be available from abstracts.
The thing is, those that are don't need their pdf searched!


My vision is shaping up as:
	Interventions in developing countries often have a direct or indirect environmental or sustainability related effect documented by studies. I extracted these effects and categorized them, using the abstract, or where insufficient information in the abstract, I attempted to recover the original document and extract a short amount of detail on the interventions, and relevant outcomes.

I need to determine:
	1. just how many relevant outcomes can be extracted from the abstracts and papers. 
	2. how many instances of each outcome are required to provide a quantifiable or categorizable result?

If I intend to use the ~50% of papers that I CAN get access to, how much does this improve the information content?


NEW IDEA: why don't I just grade the accuracy of the final result with language models?
				-> an issue with grading, if the model doesn't mention some piece of the outceom which is not really clear from the intervention prompt...
ANOTHER IDEA: Why not just specifically create another variable beyond "intervention", which is being used to prompt the models for grading? -> well... the problem is often the model will ask pointed silly questions like "was there a decrease", where the answer is obvious... however, this may be a problem with all such kinds of grading schemes.



AN issue with the idea of simply grading A, B, C, D, F:
	Sometimes it's just really obvious what the impact of a policy. Like building roads reduces forest cover... I think to capture the full range of outcomes in this way will make it really easy to guess the outcome. In a way, we wash out the distinction between mediocre, good, and great policies because great policies are the bare minimum of what you might want...



hmm

"Given this intervention and outcome, and this forecast, how well does the forecast match the actual result? Grade from: Very good, Good, Neutral/Mixed, Bad, Very bad. Justify."

this is an interesting rubric. it does not lend itself well to comparison with a random forest model.


Total outcomes: 589
Outcomes with 'No Information': 377
Fraction with 'No Information': 64.01%
Total informative outcomes: 212


so this is for the 150 in 2024-2025... So if we have 700 after 2021, then we should 



out of the 150:

Number of informative responses terms from outcomes with at least 3 informative responses: 20
Number of informative responses from outcomes with at least 3 informative responses: 92


Alright, will take about 20 dollars to get all the different outcomes and interventions categorized...


That is probably a low-ball? Anyway, if we have 700 ones past 2021, expect 2/3 of that number could be useful outcomes to grade with. So this seems reasonable. Assume 3 inflates to... 6? maybe? 6 useful outcomes?
We'll see tomorrow.

To address the issue that language models may already have information evaluating how effective a policy was, we can just report the *difference* between how it answers without any fancy prompting and aggregation and chain of thought / scratchpad, and how it answers after this.
	The issue is, these tools may be enhancing its "memory" of what it already saw about this outcome...
	Especially aggregation.

	I guess one way to test is to see how it answers on things it already knows the answer to, e.g. to policies that were published about before the knowledge cutoff date, whether these tools enhance the accuracy of what it already knows?